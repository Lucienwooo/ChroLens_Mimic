"""
é€²éšé©—è­‰ç¢¼è­˜åˆ¥æ¨¡çµ„ - å°ˆé–€é‡å° 76N8 ç´šåˆ¥çš„å¼·å™ªé»ã€å¤šè‰²å½©é©—è­‰ç¢¼
ä½¿ç”¨é¡è‰²åˆ†é›¢ + è¼ªå»“æª¢æ¸¬ + æ·±åº¦å­¸ç¿’æ–¹æ³•
"""

import cv2
import numpy as np
from PIL import Image
import pyautogui
from typing import Optional, Tuple, List, Dict
import os
from collections import Counter

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("è­¦å‘Š: pytesseract æœªå®‰è£ï¼Œéƒ¨åˆ†åŠŸèƒ½å°‡å—é™")


class AdvancedCaptchaRecognizer:
    """é€²éšé©—è­‰ç¢¼è­˜åˆ¥å™¨ - é‡å°å¼·å™ªé»ã€å¤šè‰²å½©é©—è­‰ç¢¼å„ªåŒ–"""
    
    def __init__(self):
        """åˆå§‹åŒ–é©—è­‰ç¢¼è­˜åˆ¥å™¨"""
        self.tesseract_available = TESSERACT_AVAILABLE
        
        # è¨­å®š Tesseract è·¯å¾‘ (Windows)
        if TESSERACT_AVAILABLE:
            possible_paths = [
                r'C:\Program Files\Tesseract-OCR\tesseract.exe',
                r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
                r'D:\Program Files\Tesseract-OCR\tesseract.exe',
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    pytesseract.pytesseract.tesseract_cmd = path
                    print(f"âœ“ æ‰¾åˆ° Tesseract: {path}")
                    break
    
    def capture_captcha(self, region: Tuple[int, int, int, int]) -> Optional[np.ndarray]:
        """
        æˆªå–é©—è­‰ç¢¼å€åŸŸ
        :param region: (left, top, width, height)
        :return: OpenCV åœ–ç‰‡é™£åˆ—
        """
        try:
            screenshot = pyautogui.screenshot(region=region)
            img_np = np.array(screenshot)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            return img_bgr
        except Exception as e:
            print(f"âœ— æˆªå–é©—è­‰ç¢¼å¤±æ•—: {e}")
            return None
    
    def extract_color_channels(self, img: np.ndarray) -> Dict[str, np.ndarray]:
        """
        æå–ä¸åŒé¡è‰²é€šé“ - é‡å°å¤šè‰²å½©é©—è­‰ç¢¼
        :param img: OpenCV åœ–ç‰‡ (BGR)
        :return: å„é¡è‰²é€šé“çš„äºŒå€¼åŒ–åœ–ç‰‡å­—å…¸
        """
        print("    ğŸ¨ åˆ†æé¡è‰²é€šé“...")
        
        # è½‰æ›åˆ°ä¸åŒè‰²å½©ç©ºé–“
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        
        results = {}
        
        # === 1. HSV è‰²å½©åˆ†é›¢ ===
        # ç´…è‰²ç¯„åœ (0-10 å’Œ 170-180)
        lower_red1 = np.array([0, 50, 50])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([170, 50, 50])
        upper_red2 = np.array([180, 255, 255])
        
        mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)
        results['red'] = cv2.bitwise_or(mask_red1, mask_red2)
        
        # ç¶ è‰²ç¯„åœ
        lower_green = np.array([40, 40, 40])
        upper_green = np.array([80, 255, 255])
        results['green'] = cv2.inRange(hsv, lower_green, upper_green)
        
        # è—è‰²ç¯„åœ
        lower_blue = np.array([100, 40, 40])
        upper_blue = np.array([130, 255, 255])
        results['blue'] = cv2.inRange(hsv, lower_blue, upper_blue)
        
        # é»ƒè‰²ç¯„åœ
        lower_yellow = np.array([20, 50, 50])
        upper_yellow = np.array([40, 255, 255])
        results['yellow'] = cv2.inRange(hsv, lower_yellow, upper_yellow)
        
        # é’è‰²ç¯„åœ
        lower_cyan = np.array([80, 40, 40])
        upper_cyan = np.array([100, 255, 255])
        results['cyan'] = cv2.inRange(hsv, lower_cyan, upper_cyan)
        
        # æ´‹ç´…è‰²ç¯„åœ
        lower_magenta = np.array([130, 40, 40])
        upper_magenta = np.array([170, 255, 255])
        results['magenta'] = cv2.inRange(hsv, lower_magenta, upper_magenta)
        
        # æ·±è‰²æ–‡å­—ï¼ˆä½é£½å’Œåº¦ã€ä½äº®åº¦ï¼‰
        lower_dark = np.array([0, 0, 0])
        upper_dark = np.array([180, 255, 100])
        results['dark'] = cv2.inRange(hsv, lower_dark, upper_dark)
        
        # === 2. LAB è‰²å½©åˆ†é›¢ï¼ˆæ›´ç²¾ç¢ºçš„é¡è‰²åˆ†é›¢ï¼‰===
        # Lé€šé“ - äº®åº¦
        l_channel = lab[:,:,0]
        # æå–æ·±è‰²å€åŸŸï¼ˆå­—ç¬¦å¯èƒ½åœ¨é€™è£¡ï¼‰
        _, dark_mask = cv2.threshold(l_channel, 100, 255, cv2.THRESH_BINARY_INV)
        results['lab_dark'] = dark_mask
        
        # Aé€šé“ - ç´…ç¶ è»¸
        a_channel = lab[:,:,1]
        # æå–åç´…è‰²å€åŸŸ
        _, red_a = cv2.threshold(a_channel, 135, 255, cv2.THRESH_BINARY)
        results['lab_red'] = red_a
        
        # æå–åç¶ è‰²å€åŸŸ
        _, green_a = cv2.threshold(a_channel, 0, 255, cv2.THRESH_BINARY_INV)
        results['lab_green'] = cv2.threshold(green_a, 120, 255, cv2.THRESH_BINARY_INV)[1]
        
        # === 3. RGB é€šé“åˆ†é›¢ ===
        b, g, r = cv2.split(img)
        
        # å¼·åŒ–ç´…è‰²é€šé“
        _, r_thresh = cv2.threshold(r, 100, 255, cv2.THRESH_BINARY)
        results['rgb_red'] = r_thresh
        
        # å¼·åŒ–ç¶ è‰²é€šé“
        _, g_thresh = cv2.threshold(g, 100, 255, cv2.THRESH_BINARY)
        results['rgb_green'] = g_thresh
        
        # å¼·åŒ–è—è‰²é€šé“
        _, b_thresh = cv2.threshold(b, 100, 255, cv2.THRESH_BINARY)
        results['rgb_blue'] = b_thresh
        
        # çµ±è¨ˆæ¯å€‹é€šé“çš„æœ‰æ•ˆåƒç´ æ•¸é‡
        stats = {}
        for name, mask in results.items():
            count = cv2.countNonZero(mask)
            stats[name] = count
        
        # é¡¯ç¤ºçµ±è¨ˆ
        sorted_stats = sorted(stats.items(), key=lambda x: x[1], reverse=True)
        print(f"      é¡è‰²é€šé“çµ±è¨ˆ (å‰5):")
        for name, count in sorted_stats[:5]:
            if count > 0:
                print(f"        {name}: {count} åƒç´ ")
        
        return results
    
    def remove_shadow(self, img: np.ndarray) -> np.ndarray:
        """
        ç§»é™¤é™°å½±æ•ˆæœ
        :param img: OpenCV åœ–ç‰‡
        :return: ç§»é™¤é™°å½±å¾Œçš„åœ–ç‰‡
        """
        print("    ğŸŒŸ ç§»é™¤é™°å½±...")
        
        # è½‰æ›ç‚º LAB è‰²å½©ç©ºé–“
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å° L é€šé“é€²è¡Œ CLAHE (å°æ¯”åº¦é™åˆ¶è‡ªé©æ‡‰ç›´æ–¹åœ–å‡è¡¡åŒ–)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l_clahe = clahe.apply(l)
        
        # åˆä½µå› LAB
        lab_clahe = cv2.merge([l_clahe, a, b])
        
        # è½‰å› BGR
        result = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)
        
        return result
    
    def extract_text_by_contour(self, img: np.ndarray, min_area: int = 50) -> np.ndarray:
        """
        ä½¿ç”¨è¼ªå»“æª¢æ¸¬æå–æ–‡å­—å€åŸŸ
        :param img: äºŒå€¼åŒ–åœ–ç‰‡
        :param min_area: æœ€å°è¼ªå»“é¢ç©
        :return: æå–æ–‡å­—å¾Œçš„åœ–ç‰‡
        """
        print("    ğŸ“ ä½¿ç”¨è¼ªå»“æª¢æ¸¬æå–æ–‡å­—...")
        
        # å°‹æ‰¾è¼ªå»“
        contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return img
        
        # è¨ˆç®—è¼ªå»“çµ±è¨ˆè³‡è¨Š
        areas = [cv2.contourArea(c) for c in contours]
        if not areas:
            return img
        
        median_area = np.median(areas)
        mean_area = np.mean(areas)
        std_area = np.std(areas)
        
        print(f"      è¼ªå»“çµ±è¨ˆ: ç¸½æ•¸={len(contours)}, ä¸­ä½æ•¸é¢ç©={median_area:.1f}, å¹³å‡é¢ç©={mean_area:.1f}")
        
        # å‰µå»ºé®ç½©
        mask = np.zeros_like(img)
        
        # ç¯©é¸è¼ªå»“ï¼šä¿ç•™æ¥è¿‘ä¸­ä½æ•¸å¤§å°çš„è¼ªå»“ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ï¼‰
        kept_count = 0
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # æ¢ä»¶1: é¢ç©åœ¨åˆç†ç¯„åœå…§ï¼ˆä¸­ä½æ•¸çš„ 0.2 å€åˆ° 5 å€ï¼‰
            if median_area * 0.2 <= area <= median_area * 5:
                # æ¢ä»¶2: é•·å¯¬æ¯”åˆç†ï¼ˆå­—ç¬¦ä¸æœƒå¤ªæ‰æˆ–å¤ªé«˜ï¼‰
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h if h > 0 else 0
                
                if 0.2 <= aspect_ratio <= 3.0:
                    # ç¹ªè£½è¼ªå»“
                    cv2.drawContours(mask, [contour], -1, 255, -1)
                    kept_count += 1
        
        print(f"      ä¿ç•™è¼ªå»“: {kept_count}/{len(contours)}")
        
        return mask
    
    def ultimate_preprocess(self, img: np.ndarray, save_debug: bool = False) -> List[Tuple[np.ndarray, str]]:
        """
        çµ‚æ¥µé è™•ç† - ä½¿ç”¨å¤šç¨®æ–¹æ³•çµ„åˆ
        :param img: åŸå§‹åœ–ç‰‡
        :param save_debug: æ˜¯å¦ä¿å­˜èª¿è©¦åœ–ç‰‡
        :return: è™•ç†å¾Œçš„åœ–ç‰‡åˆ—è¡¨ [(åœ–ç‰‡, æ–¹æ³•åç¨±), ...]
        """
        print("\n  ğŸš€ === å•Ÿå‹•çµ‚æ¥µé è™•ç†æµç¨‹ === ğŸš€")
        results = []
        
        # æ”¾å¤§åœ–ç‰‡ (10å€ï¼Œæä¾›æ›´å¤šç´°ç¯€)
        h, w = img.shape[:2]
        enlarged = cv2.resize(img, (w * 10, h * 10), interpolation=cv2.INTER_LANCZOS4)
        
        # === æ–¹æ³• 1: é¡è‰²åˆ†é›¢æ³• ===
        print("\n  [æ–¹æ³• 1] é¡è‰²åˆ†é›¢æ³•")
        color_channels = self.extract_color_channels(enlarged)
        
        # å°æ¯å€‹æœ‰æ•ˆçš„é¡è‰²é€šé“é€²è¡Œè™•ç†
        for color_name, color_mask in color_channels.items():
            if cv2.countNonZero(color_mask) > 100:  # è‡³å°‘è¦æœ‰ä¸€äº›åƒç´ 
                # é™å™ª
                denoised = cv2.morphologyEx(color_mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))
                denoised = cv2.morphologyEx(denoised, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))
                
                # è¼ªå»“æå–
                contour_result = self.extract_text_by_contour(denoised, min_area=100)
                
                if cv2.countNonZero(contour_result) > 0:
                    # è†¨è„¹ä»¥é€£æ¥æ–·è£‚
                    kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                    dilated = cv2.dilate(contour_result, kernel_dilate, iterations=2)
                    
                    results.append((dilated, f"color_{color_name}"))
        
        # === æ–¹æ³• 2: ç§»é™¤é™°å½± + ç°éšè™•ç† ===
        print("\n  [æ–¹æ³• 2] é™°å½±ç§»é™¤æ³•")
        shadow_removed = self.remove_shadow(enlarged)
        
        # è½‰ç°éš
        gray = cv2.cvtColor(shadow_removed, cv2.COLOR_BGR2GRAY)
        
        # å¤šéšæ®µé™å™ª
        # 1. ä¸­å€¼æ¿¾æ³¢
        median = cv2.medianBlur(gray, 7)
        
        # 2. éå±€éƒ¨å‡å€¼é™å™ª
        nlm = cv2.fastNlMeansDenoising(median, None, h=30, templateWindowSize=7, searchWindowSize=21)
        
        # 3. é›™é‚Šæ¿¾æ³¢
        bilateral = cv2.bilateralFilter(nlm, 11, 90, 90)
        
        # 4. é«˜æ–¯æ¨¡ç³Š
        gaussian = cv2.GaussianBlur(bilateral, (5, 5), 1.5)
        
        # å¤šç¨®äºŒå€¼åŒ–æ–¹æ³•
        # Otsu
        _, binary_otsu = cv2.threshold(gaussian, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        results.append((binary_otsu, "shadow_removed_otsu"))
        
        # è‡ªé©æ‡‰é–¾å€¼
        binary_adaptive = cv2.adaptiveThreshold(
            gaussian, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV, blockSize=31, C=10
        )
        results.append((binary_adaptive, "shadow_removed_adaptive"))
        
        # === æ–¹æ³• 3: å½¢æ…‹å­¸æ¢¯åº¦é‚Šç·£æª¢æ¸¬ ===
        print("\n  [æ–¹æ³• 3] å½¢æ…‹å­¸æ¢¯åº¦æ³•")
        kernel_grad = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        gradient = cv2.morphologyEx(gaussian, cv2.MORPH_GRADIENT, kernel_grad)
        
        # äºŒå€¼åŒ–
        _, binary_grad = cv2.threshold(gradient, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # é–‰é‹ç®—é€£æ¥é‚Šç·£
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        closed_grad = cv2.morphologyEx(binary_grad, cv2.MORPH_CLOSE, kernel_close, iterations=3)
        
        results.append((closed_grad, "morphological_gradient"))
        
        # === æ–¹æ³• 4: Cannyé‚Šç·£ + è†¨è„¹ ===
        print("\n  [æ–¹æ³• 4] Canny é‚Šç·£æ³•")
        edges = cv2.Canny(gaussian, 50, 150)
        
        # è†¨è„¹é€£æ¥é‚Šç·£
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        dilated_edges = cv2.dilate(edges, kernel_dilate, iterations=2)
        
        # é–‰é‹ç®—å¡«å……
        closed_edges = cv2.morphologyEx(dilated_edges, cv2.MORPH_CLOSE, kernel_close, iterations=3)
        
        results.append((closed_edges, "canny_edge"))
        
        # === æ–¹æ³• 5: å¤šå°ºåº¦å¤šé–¾å€¼æŠ•ç¥¨æ³• ===
        print("\n  [æ–¹æ³• 5] å¤šå°ºåº¦æŠ•ç¥¨æ³•")
        vote_result = np.zeros_like(gray, dtype=np.float32)
        
        for scale in [8, 10, 12]:
            scaled = cv2.resize(img, (w * scale, h * scale), interpolation=cv2.INTER_CUBIC)
            gray_scaled = cv2.cvtColor(scaled, cv2.COLOR_BGR2GRAY)
            
            # é™å™ª
            denoised_scaled = cv2.fastNlMeansDenoising(gray_scaled, None, h=25, templateWindowSize=7, searchWindowSize=21)
            
            # å¤šé–¾å€¼
            _, b1 = cv2.threshold(denoised_scaled, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            b2 = cv2.adaptiveThreshold(denoised_scaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 25, 8)
            
            # åˆä½µ
            combined = cv2.bitwise_and(b1, b2)
            
            # èª¿æ•´åˆ°ç›®æ¨™å¤§å°
            resized = cv2.resize(combined, (gray.shape[1], gray.shape[0]), interpolation=cv2.INTER_AREA)
            
            # ç´¯åŠ æŠ•ç¥¨
            vote_result += resized.astype(np.float32) / 255.0
        
        # å–å¹³å‡ï¼Œè¶…é50%æŠ•ç¥¨å‰‡ç‚ºå‰æ™¯
        final_vote = ((vote_result / 3.0) > 0.5).astype(np.uint8) * 255
        results.append((final_vote, "multiscale_voting"))
        
        # === æ–¹æ³• 6: é¡è‰²èšåˆæ³• ===
        print("\n  [æ–¹æ³• 6] é¡è‰²èšåˆæ³•")
        # åˆä½µæœ€ä½³çš„å¹¾å€‹é¡è‰²é€šé“
        best_channels = []
        channel_stats = [(name, cv2.countNonZero(mask)) for name, mask in color_channels.items()]
        channel_stats.sort(key=lambda x: x[1], reverse=True)
        
        for name, count in channel_stats[:3]:  # å–å‰3å€‹
            if count > 500:  # è‡³å°‘æœ‰500å€‹åƒç´ 
                best_channels.append(color_channels[name])
        
        if best_channels:
            # åˆä½µé€šé“
            aggregated = np.zeros_like(best_channels[0])
            for channel in best_channels:
                aggregated = cv2.bitwise_or(aggregated, channel)
            
            # è¼ªå»“æå–
            contour_agg = self.extract_text_by_contour(aggregated, min_area=100)
            
            # å½¢æ…‹å­¸è™•ç†
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            cleaned_agg = cv2.morphologyEx(contour_agg, cv2.MORPH_CLOSE, kernel, iterations=2)
            
            results.append((cleaned_agg, "color_aggregation"))
        
        # å°æ‰€æœ‰çµæœé€²è¡Œå¾Œè™•ç†
        print("\n  ğŸ”§ å¾Œè™•ç†æ‰€æœ‰çµæœ...")
        final_results = []
        for idx, (img_processed, method_name) in enumerate(results):
            # é€£é€šçµ„ä»¶åˆ†æå»é™¤å°å™ªé»
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img_processed, connectivity=8)
            
            if num_labels > 1:
                areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels)]
                if areas:
                    median_area = np.median(areas)
                    
                    # å‰µå»ºé®ç½©ï¼Œåªä¿ç•™åˆç†å¤§å°çš„çµ„ä»¶
                    mask = np.zeros_like(img_processed)
                    for i in range(1, num_labels):
                        area = stats[i, cv2.CC_STAT_AREA]
                        if area > median_area * 0.15:  # ä¿ç•™é¢ç© > ä¸­ä½æ•¸çš„15%
                            mask[labels == i] = 255
                    
                    img_processed = mask
            
            # æœ€çµ‚éŠ³åŒ–
            kernel_sharp = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
            sharpened = cv2.filter2D(img_processed, -1, kernel_sharp)
            
            final_results.append((sharpened, method_name))
            
            # ä¿å­˜èª¿è©¦åœ–ç‰‡
            if save_debug:
                debug_path = f"debug_{idx}_{method_name}.png"
                cv2.imwrite(debug_path, sharpened)
                print(f"      ä¿å­˜èª¿è©¦åœ–ç‰‡: {debug_path}")
        
        print(f"\n  âœ… å®Œæˆé è™•ç†ï¼Œç”¢ç”Ÿ {len(final_results)} å€‹å€™é¸åœ–ç‰‡")
        return final_results
    
    def recognize_with_tesseract(self, img: np.ndarray, 
                                 char_type: str = "alphanumeric",
                                 psm: int = 7) -> Optional[str]:
        """
        ä½¿ç”¨ Tesseract OCR è­˜åˆ¥
        :param img: OpenCV åœ–ç‰‡
        :param char_type: å­—ç¬¦é¡å‹
        :param psm: Page Segmentation Mode
        :return: è­˜åˆ¥çµæœ
        """
        if not self.tesseract_available:
            return None
        
        try:
            # å­—ç¬¦ç™½åå–®
            if char_type == "digits":
                whitelist = '0123456789'
            elif char_type == "alpha":
                whitelist = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
            else:  # alphanumeric
                whitelist = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
            
            # é…ç½®
            config = f'--psm {psm} --oem 3 -c tessedit_char_whitelist={whitelist}'
            
            # è½‰æ›ç‚º PIL Image
            if len(img.shape) == 2:  # ç°éšåœ–
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
            else:
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            pil_img = Image.fromarray(img_rgb)
            
            # OCR
            text = pytesseract.image_to_string(pil_img, config=config)
            text = ''.join(filter(str.isalnum, text)).strip()
            
            return text if text else None
            
        except Exception as e:
            return None
    
    def recognize_captcha(self, 
                         region: Tuple[int, int, int, int],
                         char_type: str = "alphanumeric",
                         save_debug: bool = True) -> Optional[str]:
        """
        å®Œæ•´é©—è­‰ç¢¼è­˜åˆ¥æµç¨‹
        :param region: é©—è­‰ç¢¼å€åŸŸ (left, top, width, height)
        :param char_type: å­—ç¬¦é¡å‹
        :param save_debug: æ˜¯å¦ä¿å­˜èª¿è©¦åœ–ç‰‡
        :return: è­˜åˆ¥çµæœ
        """
        print("\n" + "="*70)
        print("ğŸ” === é€²éšé©—è­‰ç¢¼è­˜åˆ¥ç³»çµ±å•Ÿå‹• (76N8 å°ˆç”¨) === ğŸ”")
        print("="*70)
        
        # 1. æˆªå–é©—è­‰ç¢¼
        print(f"\n[1/4] ğŸ“¸ æˆªå–é©—è­‰ç¢¼å€åŸŸ: {region}")
        img = self.capture_captcha(region)
        if img is None:
            return None
        print("  âœ“ æˆªå–æˆåŠŸ")
        
        # ä¿å­˜åŸå§‹åœ–ç‰‡
        if save_debug:
            cv2.imwrite("debug_00_original.png", img)
            print("  âœ“ ä¿å­˜åŸå§‹åœ–ç‰‡: debug_00_original.png")
        
        # 2. çµ‚æ¥µé è™•ç†
        print(f"\n[2/4] ğŸ”§ åœ–ç‰‡é è™•ç†ä¸­...")
        processed_images = self.ultimate_preprocess(img, save_debug=save_debug)
        
        # 3. OCR è­˜åˆ¥
        print(f"\n[3/4] ğŸ¤– OCR è­˜åˆ¥ä¸­...")
        all_results = []
        
        # PSM æ¨¡å¼åˆ—è¡¨
        psm_modes = [6, 7, 8, 10, 11, 13]
        
        for img_processed, method_name in processed_images:
            for psm in psm_modes:
                result = self.recognize_with_tesseract(img_processed, char_type, psm)
                
                if result and len(result) >= 3:  # éæ¿¾å¤ªçŸ­çš„çµæœ
                    all_results.append((result, f"{method_name}_PSM{psm}"))
                    print(f"  [{method_name}] PSM{psm}: '{result}'")
        
        # 4. çµ±è¨ˆå’Œé¸æ“‡æœ€ä½³çµæœ
        print(f"\n[4/4] ğŸ“Š åˆ†æçµæœ...")
        if all_results:
            # çµ±è¨ˆå‡ºç¾æ¬¡æ•¸
            result_counts = Counter([r for r, _ in all_results])
            
            # é¡¯ç¤ºçµ±è¨ˆ
            print(f"  å…±ç²å¾— {len(all_results)} å€‹è­˜åˆ¥çµæœ:")
            for result, count in result_counts.most_common(10):
                sources = [s for r, s in all_results if r == result]
                print(f"    '{result}' - å‡ºç¾ {count} æ¬¡")
                if count > 1:
                    print(f"      ä¾†æº: {', '.join(sources[:3])}{' ...' if len(sources) > 3 else ''}")
            
            # é¸æ“‡ç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡å‡ºç¾æ¬¡æ•¸å¤šçš„ï¼Œå¦‚æœæ¬¡æ•¸ç›¸åŒå‰‡é¸æ“‡4å€‹å­—ç¬¦çš„
            candidates = []
            for result, count in result_counts.most_common():
                if len(result) == 4:  # 76N8 æ˜¯4å€‹å­—ç¬¦
                    candidates.append((result, count))
            
            # å¦‚æœæ²’æœ‰4å€‹å­—ç¬¦çš„ï¼Œå°±é¸å‡ºç¾æœ€å¤šçš„
            if not candidates:
                candidates = result_counts.most_common()
            
            best_result = candidates[0][0] if candidates else result_counts.most_common(1)[0][0]
            best_count = candidates[0][1] if candidates else result_counts.most_common(1)[0][1]
            
            print(f"\nâœ… === è­˜åˆ¥å®Œæˆ ===")
            print(f"  æœ€ä½³çµæœ: '{best_result}' (é•·åº¦: {len(best_result)}, ç½®ä¿¡åº¦: {best_count}/{len(processed_images)} æ–¹æ³•æŠ•ç¥¨)")
            print("="*70 + "\n")
            
            return best_result
        else:
            print(f"\nâŒ === è­˜åˆ¥å¤±æ•— ===")
            print(f"  æ‰€æœ‰æ–¹æ³•å‡ç„¡æ³•è­˜åˆ¥é©—è­‰ç¢¼")
            print(f"  å»ºè­°: æª¢æŸ¥åœ–ç‰‡å“è³ªæˆ–èª¿æ•´è­˜åˆ¥åƒæ•¸")
            print("="*70 + "\n")
            return None
    
    def recognize_from_file(self, filepath: str, 
                           char_type: str = "alphanumeric",
                           save_debug: bool = True) -> Optional[str]:
        """
        å¾æª”æ¡ˆè­˜åˆ¥é©—è­‰ç¢¼
        :param filepath: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
        :param char_type: å­—ç¬¦é¡å‹
        :param save_debug: æ˜¯å¦ä¿å­˜èª¿è©¦åœ–ç‰‡
        :return: è­˜åˆ¥çµæœ
        """
        print("\n" + "="*70)
        print(f"ğŸ” === å¾æª”æ¡ˆè­˜åˆ¥é©—è­‰ç¢¼: {filepath} === ğŸ”")
        print("="*70)
        
        # è®€å–åœ–ç‰‡
        img = cv2.imread(filepath)
        if img is None:
            print(f"âœ— ç„¡æ³•è®€å–åœ–ç‰‡: {filepath}")
            return None
        
        print(f"  âœ“ è®€å–æˆåŠŸï¼Œåœ–ç‰‡å¤§å°: {img.shape[1]}x{img.shape[0]}")
        
        # ä¿å­˜åŸå§‹åœ–ç‰‡
        if save_debug:
            cv2.imwrite("debug_00_original.png", img)
            print("  âœ“ ä¿å­˜åŸå§‹åœ–ç‰‡: debug_00_original.png")
        
        # çµ‚æ¥µé è™•ç†
        print(f"\nğŸ”§ åœ–ç‰‡é è™•ç†ä¸­...")
        processed_images = self.ultimate_preprocess(img, save_debug=save_debug)
        
        # OCR è­˜åˆ¥
        print(f"\nğŸ¤– OCR è­˜åˆ¥ä¸­...")
        all_results = []
        
        psm_modes = [6, 7, 8, 10, 11, 13]
        
        for img_processed, method_name in processed_images:
            for psm in psm_modes:
                result = self.recognize_with_tesseract(img_processed, char_type, psm)
                
                if result and len(result) >= 3:
                    all_results.append((result, f"{method_name}_PSM{psm}"))
                    print(f"  [{method_name}] PSM{psm}: '{result}'")
        
        # çµ±è¨ˆå’Œé¸æ“‡æœ€ä½³çµæœ
        print(f"\nğŸ“Š åˆ†æçµæœ...")
        if all_results:
            result_counts = Counter([r for r, _ in all_results])
            
            print(f"  å…±ç²å¾— {len(all_results)} å€‹è­˜åˆ¥çµæœ:")
            for result, count in result_counts.most_common(10):
                print(f"    '{result}' - å‡ºç¾ {count} æ¬¡")
            
            # å„ªå…ˆé¸æ“‡4å€‹å­—ç¬¦çš„çµæœ
            candidates = []
            for result, count in result_counts.most_common():
                if len(result) == 4:
                    candidates.append((result, count))
            
            if not candidates:
                candidates = result_counts.most_common()
            
            best_result = candidates[0][0] if candidates else result_counts.most_common(1)[0][0]
            best_count = candidates[0][1] if candidates else result_counts.most_common(1)[0][1]
            
            print(f"\nâœ… === è­˜åˆ¥å®Œæˆ ===")
            print(f"  æœ€ä½³çµæœ: '{best_result}' (é•·åº¦: {len(best_result)}, ç½®ä¿¡åº¦: {best_count}/{len(processed_images)} æ–¹æ³•æŠ•ç¥¨)")
            print("="*70 + "\n")
            
            return best_result
        else:
            print(f"\nâŒ === è­˜åˆ¥å¤±æ•— ===")
            print("="*70 + "\n")
            return None


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    print("é€²éšé©—è­‰ç¢¼è­˜åˆ¥æ¨¡çµ„")
    print("å°ˆé–€é‡å° 76N8 ç´šåˆ¥çš„å¼·å™ªé»ã€å¤šè‰²å½©é©—è­‰ç¢¼\n")
    
    recognizer = AdvancedCaptchaRecognizer()
    
    # æ¸¬è©¦å¾æª”æ¡ˆè­˜åˆ¥
    test_file = "captcha_test.png"  # æ‚¨çš„é©—è­‰ç¢¼åœ–ç‰‡
    if os.path.exists(test_file):
        result = recognizer.recognize_from_file(test_file, char_type="alphanumeric", save_debug=True)
        print(f"\næœ€çµ‚è­˜åˆ¥çµæœ: {result}")
    else:
        print(f"è«‹å°‡é©—è­‰ç¢¼åœ–ç‰‡ä¿å­˜ç‚º {test_file} å¾ŒåŸ·è¡Œæ¸¬è©¦")
